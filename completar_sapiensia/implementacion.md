Análisis Integral del Sistema SapiensIA y Plan de Implementación
1. Verificación de Módulos Virtuales y Arquitectura Actual
Estado Actual: La arquitectura frontend-driven descrita en la documentación unificada está en gran medida implementada. En el backend existen modelos para VirtualModule, VirtualTopic y VirtualTopicContent que reflejan la jerarquía Plan de Estudio → Módulo → Tema → Contenido Virtual[1][2]. El backend actúa como almacén de datos crudos: expone los contenidos originales del profesor (TopicContent con campos como personalization_markers y slide_template) y guarda las instancias virtuales para cada estudiante. El frontend realiza la lógica avanzada de presentación:
•	Fragmentación de diapositivas: El hook React useTopicOrchestrator toma el HTML consolidado de las diapositivas (content_type = 'slides') y lo divide en láminas individuales mediante DOMParser[3]. Cada fragmento hereda la plantilla de diapositiva original (se extrae y aplica vía original_slide_template)[4].
•	Secuenciación con IA: Luego compone una lista de “átomos de contenido” (resúmenes breves de cada fragmento y de cada contenido adicional) y llama a un servicio de IA (e.g. getAiOrderedSequence) para obtener un orden pedagógico óptimo[5]. El sistema efectivamente reordena los fragmentos según los IDs devueltos por la IA[6]. Esto confirma que la orquestación progresiva en frontend está en marcha: inicialmente se muestra el contenido en orden por defecto, y al llegar la secuencia óptima se re-renderiza en el nuevo orden de forma fluida (como indica el código, si la IA falla se mantiene el orden original)[7].
•	Personalización de marcadores: Cada elemento se renderiza con ContentRenderer, que usa el hook useContentPersonalization. Este hook identifica los marcadores {{...}} en el contenido y (simulado por ahora) obtiene reemplazos personalizados de IA[8][9]. Aplica recursivamente los reemplazos tanto en strings HTML como en objetos JSON complejos[10][11]. Actualmente la implementación usa una función simulada (fetchIaReplacements) que devuelve valores dummy (por ejemplo, colores aleatorios o el nombre "Alex")[12], pero la estructura está lista para conectarse a un servicio real de IA. Si no hay marcadores, el contenido se deja tal cual[13].
Conclusión: La presentación de contenidos virtuales sigue el flujo descrito en la documentación: el frontend solicita al backend los VirtualTopicContent de un tema mediante GET /api/virtual/topic/<id>/contents, recibe todos los contenidos originales con sus marcadores y plantillas, fragmenta y muestra de inmediato, y luego aplica secuenciación y personalización en segundo plano[14][6]. Los campos enviados por el backend confirman esto: por cada contenido virtual se incluyen original_content, original_personalization_markers y original_slide_template extraídos del contenido del profesor[15].
Pendientes/Verificación: Se debe confirmar que todos los tipos de contenido de un tema se están entregando en ese endpoint y que el frontend realmente los incluye en la secuenciación. El código actual no filtra contenidos según perfil, por lo que actualmente todos los contenidos publicados de un tema se copian al VirtualTopic (ver más en la sección de personalización). Además, verificar que el campo locked del tema virtual funciona: en la creación rápida se marca locked=False para los primeros temas generados[16], y en generación por avance también se establecen los nuevos temas con locked: False[17], lo cual difiere de la idea original de mantenerlos bloqueados hasta completar el previo. Es necesario alinear esta lógica con la UX deseada (por ejemplo, mantener temas “en cola” pero no accesibles hasta que corresponda desbloquearlos).
2. Personalización de Contenidos y Selección Adaptativa
Una de las metas clave es que el contenido se adapte al perfil cognitivo del estudiante en dos niveles: (a) seleccionando los tipos de contenido más adecuados para su estilo de aprendizaje, y (b) ajustando detalles internos del contenido (p. ej., nombres, contextos) mediante IA.
•	(a) Selección de tipos de contenido: Actualmente, el backend no filtra contenidos por perfil en la generación de contenidos virtuales; inserta todos los contenidos originales de un tema en la colección virtual_topic_contents para el alumno[18][19]. Vemos que en _generate_topic_contents_for_sync se recorre cada TopicContent original y se crea un VirtualTopicContent correspondiente sin discriminación, simplemente marcándolo con el perfil en personalization_data.adapted_for_profile[19]. Esto significa que hoy el estudiante recibe todos los tipos de recursos (texto, diapositivas, video, quiz, etc.) disponibles del tema, en lugar de una selección óptima. La idea original era elegir solo aquellos formatos que mejor se adapten a su estilo de aprendizaje (visual, auditivo, kinestésico, etc.).
Recomendación: Implementar un filtrado basado en el perfil antes de insertar contenidos virtuales. Por ejemplo, cada TopicContent tiene una lista de learning_methodologies compatibles (en su modelo) y hay definiciones de metodologías y su match con perfiles cognitivos[20][21]. Se podría definir para cada estudiante un subconjunto de tipos preferidos; al generar el módulo virtual, incluir siempre al menos un contenido teórico completo (ej. slides o texto) para asegurar cobertura total del tema, y luego agregar otros tipos solo si calzan con las preferencias del perfil. Si un tema carece de formatos en la preferencia del alumno, se incluyen los necesarios de todas formas para no dejar huecos en el contenido (balance entre personalización y cobertura). Esta lógica aún no existe y deberá agregarse en la función de generación de contenidos virtuales: - Filtrar original_contents según el perfil (cognitive_profile). Por ejemplo, si el perfil indica estilo visual, priorizar contenidos cuyos learning_methodologies incluyan "visual"[20][22]; si es gamification, incluir juegos/simulaciones (content_type de categoría interactive) siempre que existan, etc. - Garantizar cobertura: asegurar que al menos un contenido “completo” (texto extenso, diapositivas o video) esté presente aunque no sea el preferido, porque algunos tipos (ej. un juego o infografía) solo cubren parte de la materia. La estrategia puede ser: siempre incluir el material base (por ejemplo diapositivas generadas con método Feynman), y luego añadir 2-3 recursos adicionales óptimos según perfil. - Marcar en personalization_data qué criterios se usaron (p.ej., "selected_by": "visual_profile"), para fines de depuración o ajustes futuros.
•	(b) Personalización de contenido interno: En este aspecto, la base está sentada. Los marcadores de personalización ({{...}}) se guardan en cada TopicContent.personalization_markers y se transmiten al frontend[15]. El hook de React encuentra esos marcadores y llama a la IA para reemplazarlos[23][8]. Actualmente la llamada es simulada; se debe conectar a un endpoint real de IA (posiblemente un modelo como GPT) que, con base en el perfil del estudiante (edad, intereses, contexto previo), devuelva valores concretos. Por ejemplo, {{nombre}} → nombre del estudiante, {{color}} → su color favorito, {{ciudad}} → ciudad natal, etc.
Verificación: Hay que validar que el modelo de marcadores está consistente entre backend y frontend. El código frontend espera que original_personalization_markers tenga una estructura con segments y campos id y type = 'marker'[23]. Esto sugiere que la generación de contenidos teóricos ya inserta marcadores usando ese formato (por ejemplo, durante la generación del texto teórico con Método Feynman, se podrían haber incluido placeholders como {{nombre}}). Sería prudente revisar un ejemplo real de TopicContent.personalization_markers guardado en la BD para asegurarse de que coincide con lo que el frontend espera. En caso contrario, ajustar uno u otro.
Próximos pasos: Integrar el perfil cognitivo en la generación de los reemplazos. Actualmente se usa un MOCK_STUDENT_PROFILE fijo[24] para la secuenciación IA, y en personalización no se envía contexto real del estudiante. Deberíamos pasar atributos relevantes (p. ej. nombre, preferencias, nivel educativo) al servicio de IA de personalización. Esto permitirá que los reemplazos sean realmente personalizados (e.g., incluir el nombre del estudiante en un enunciado, o ajustar la dificultad de un ejemplo según su nivel).
•	Aprendizaje por refuerzo del perfil: No está implementado, pero es una excelente mejora futura. La idea es retroalimentar el perfil del estudiante con los resultados de su interacción (los ContentResult, ver sección 3) para refinar la selección de contenidos. Por ejemplo, si consistentemente obtiene mejores puntajes en quizzes que en juegos, quizá prefiera evaluaciones formales sobre dinámicas – el sistema podría aumentar la proporción de quizzes. Se puede diseñar un motor complementario que analice los ContentResult de un estudiante por tipo de contenido: calculando eficacia (score promedio) y engagement (tiempo dedicado, número de intentos) para cada tipo. Luego, actualizar un campo en el perfil (p. ej., profile.preferred_modalities) para reflejar esos hallazgos. Esto debe hacerse con cuidado:
•	Inicialmente, no quitar contenidos basándose en esto, sino añadir más del tipo preferido o presentarlos primero.
•	Necesitaríamos programar un proceso (tal vez al finalizar un módulo, o al terminar cada tema) que compile los resultados y llame a un servicio IA o lógica interna para decidir ajustes al perfil.
•	Como dice el usuario, este motor de refuerzo debe complementar al perfil inicial (no reemplazarlo). Sugerimos almacenarlo por separado, quizá en student.profile.reinforcement_data para luego combinar con el perfil base en la selección de contenidos.
3. Almacenamiento de Resultados de Contenido (ContentResult)
Estado Actual: El sistema cuenta con un modelo unificado ContentResult para registrar los resultados de interacción del estudiante con cualquier contenido (quiz, juego, lectura, etc.)[25][26]. Cada ContentResult guarda: - content_id: el ID del contenido original (TopicContent) con el que se interactuó[27], - student_id: el estudiante que lo realizó, - score: una puntuación numérica (posiblemente de 0 a 1 o 0 a 100 según el caso), - feedback: texto opcional (ej. explicación de la IA o retroalimentación del profesor), - metrics: dict opcional para detalles (tiempo empleado, número de intentos, etc.), - session_type: tipo de sesión (por defecto "content_interaction", pero podría usarse otro valor para distinguir, por ejemplo, interacciones de chatbot, evaluaciones manuales, etc.).
Asociación a contenido original vs virtual: Actualmente, content_id se está guardando como referencia al contenido original (TopicContent), no al VirtualTopicContent. Esto significa que si 5 estudiantes hacen el mismo quiz (derivado del mismo contenido original), habrá 5 ContentResults distintos apuntando al mismo content_id. Esta decisión de diseño unifica los resultados por contenido base, lo cual facilita al profesor obtener estadísticas globales (e.g., “el quiz X tuvo 80% de aciertos promedio en la clase”) sin tener que agregar manualmente. Ventaja: simplifica análisis grupal y vinculación con evaluaciones (ver más abajo). Desventaja: no distingue variaciones personalizadas (si la personalización cambiara significativamente la pregunta por estudiante, aunque en teoría los marcadores no alteran la esencia evaluativa). En general, mantener la referencia al contenido original es lógico y suficiente[27]; no parece necesario crear un ContentResult por contenido virtual salvo casos muy específicos.
Verificación: Conviene confirmar en el frontend cómo se registra un ContentResult. Por ejemplo, cuando un estudiante completa un quiz, ¿se llama a un endpoint para guardar el resultado? Dado que el sistema unificó quizzes en TopicContent, es probable que exista una ruta tipo POST /api/content/result o similar que crea estos registros. También se observa que en el modelo StudyPlan se eliminó un antiguo EvaluationResult a favor de ContentResult[28], evidenciando la intención de usar ContentResult para todas las evaluaciones, incluso las generales del módulo.
Uso según tipo de contenido: Debe definirse la convención de score por tipo: - Lectura/Video: al no haber nota objetiva, la convención podría ser score = 1.0 (100%) al marcar el contenido como visto. De hecho, el tracking de VirtualTopicContent.interaction_tracking tiene completion_percentage y estado (not_started, completed)[29][30], por lo que podríamos no crear un ContentResult para lecturas a menos que se quiera dejar constancia de tiempo de lectura u otras métricas. Una opción es solo marcar completado sin score. - Quiz: calcular el porcentaje de aciertos (ej. 0.8 si sacó 4/5). Este 0–1 luego puede multiplicarse por la ponderación de la evaluación formal si corresponde. - Juego/Simulación: muchos juegos generan puntaje; habría que normalizarlo a 0–1 respecto a un máximo posible o usar la métrica para registrar puntos crudos en metrics. Alternativamente, score podría ser binario (1 si completó con éxito los objetivos del juego, 0 si no). - Entrega (proyecto): en caso de entregables evaluados manual o automáticamente, score será la calificación asignada (0–1 o escalada a 100). Si la evaluación es cualitativa, quizá solo feedback sin score, pero lo ideal es cuantificar para fines de promedio.
Implementación Adicional: Es importante automatizar la creación de ContentResult en distintos escenarios: - Cuando un estudiante termina un quiz en el frontend, llamar a la API para guardar resultado. Igualmente con juegos (si la lógica existe para detectar finalización). - Marcar contenido visto: podría crearse un ContentResult con score 1 y quizás session_type: "view" o simplemente actualizar el completion_status en VirtualTopicContent. Probablemente basta con el tracking interno para lecturas. - Consolidar progreso del tema usando estos resultados: el backend ya calcula progreso por tema (VirtualTopic.progress) como la fracción de contenidos completados[31]. Asegurarse que esta actualización de progreso ocurra bien al marcar contenidos como completados (podría actualizarse al insertar ContentResult o al marcar completion_status en VirtualTopicContent).
Nota: Actualmente, VirtualTopicContent guarda un campo interaction_tracking que duplica parte de la información (estado completado, mejor score)[32]. Este es útil para presentación inmediata (por ejemplo, mostrar “completado” con check verde en la UI sin tener que consultar ContentResult). Sin embargo, la fuente de verdad para notas debe ser ContentResult. Se recomienda: - Mantener VirtualTopicContent.interaction_tracking sincronizado con ContentResult (e.g., al guardar un ContentResult de quiz con score, actualizar best_score y completion_status = completed en el VirtualTopicContent correspondiente). - En evaluaciones formales de módulo (que abarcan varios contenidos), usar ContentResults para cálculos de calificaciones (ver siguiente sección).
4. Sistema de Evaluaciones: Ponderaciones, Vinculación de Contenidos y Entregables
Las evaluaciones de módulo son componentes importantes y actualmente cuentan con un modelo base en backend: - Evaluation asociado a un módulo[33], con campos: título, descripción, peso (en % o puntos), criterios (arreglo de criterios/rúbrica), fecha de entrega, y flags: - use_quiz_score: booleano que indica si la nota proviene de un quiz automático[34], - requires_submission: booleano que indica si requiere entrega de un archivo[35], - linked_quiz_id: referencia al contenido (TopicContent) de tipo quiz cuyo resultado se tomará si use_quiz_score=true[35].
•	EvaluationResource: modelo que vincula una Evaluation con un Resource (archivo) indicando su rol: "template" (plantilla o enunciado), "supporting_material" (material de apoyo o rúbrica), o "submission" (entrega del estudiante)[36][37].
Estado Actual: Esta estructura cubre en gran medida las necesidades planteadas: - Evaluaciones asociadas a múltiples temas: Aunque cada Evaluation pertenece a un único módulo, nada impide que en su campo criteria o descripción se indique que abarca cierto conjunto de temas del módulo. La flexibilidad para abarcar varios temas dentro del mismo módulo está dada por el campo criteria (podría listar, por ejemplo, “Tema1: 50%, Tema2: 50%”). Sin embargo, no existe asociación explícita a temas individuales ni a múltiples módulos. Si se requiere evaluar conjuntamente temas de distintos módulos, actualmente no es posible con un solo objeto Evaluation (tendría que crearse una evaluación en cada módulo y tratarlos como vinculados manualmente, lo cual es engorroso). Dado que usualmente las evaluaciones formales se circunscriben a un curso (módulo) a la vez, esta limitación es aceptable. Podemos documentar que una Evaluación pertenece a un módulo, pero en su contenido puede cubrir varios temas de ese módulo según la configuración del profesor. - Ponderación manual vs automática: El modelo admite: - Manual: El profesor no activa use_quiz_score ni requires_submission. En este caso la evaluación sería, por ejemplo, una exposición o proyecto en clase. La nota la introduciría el profesor manualmente. Implementación: Aquí faltarían UI y endpoints para que el docente ingrese la calificación de cada estudiante. Podríamos reutilizar ContentResult para almacenar esta nota manual: crear un ContentResult con session_type: "evaluation" (por ejemplo) enlazado a algún ID representativo. Dado que EvaluationResult fue deprecado, quizás la idea es justamente usar ContentResult también para evaluaciones. Alternativamente, se podría reinstaurar un EvaluationResult simple (evaluation_id, student_id, score) para estas ocasiones, pero no es necesario duplicar si ContentResult puede servir. Una propuesta: cuando el profesor introduce la nota manual de una Evaluación, crear un ContentResult cuyo content_id sea la Evaluation misma (tendríamos que decidir si usar un ID ficticio, o extender ContentResult para aceptar evaluation_id en lugar de content_id). Mejor solución: añadir a ContentResult un campo opcional evaluation_id, o simplemente usar un content_id especial si consideramos cada Evaluación como un “contenido evaluativo global”. - Quiz automático: Si use_quiz_score=true, el sistema debe tomar la nota directamente del ContentResult del quiz enlazado. Vemos que linked_quiz_id espera un ID de Quiz (TopicContent). La implementación prevista sería: cuando el estudiante completa ese quiz, se genera su ContentResult. Al momento de calcular la nota de la Evaluación, simplemente se busca ContentResult donde content_id = linked_quiz_id y student_id = X, y de allí el score. Falta: código para realizar esta asociación automática al mostrar notas o cerrar el módulo. También sería conveniente guardar, al momento de crear la Evaluación, algún tipo de vínculo para cálculos rápidos (aunque con un índice en ContentResult por content_id es suficiente). - Entrega (proyecto/trabajo): Si requires_submission=true, significa que cada estudiante deberá subir un archivo. El flujo deseado es: 1. El profesor opcionalmente adjunta recursos de enunciado o guía (por ej., un PDF con preguntas, o una rúbrica) a la Evaluación. Esto ya se puede hacer creando un Resource (subida de archivo) y luego un EvaluationResource con role "template" o "supporting_material"[36][37]. La UI del profesor debería permitir adjuntar estos archivos al configurar la evaluación. 2. El estudiante, antes de la fecha límite, sube su archivo de respuesta. Esa subida creará un Resource (propietario = estudiante) y un EvaluationResource con role "submission". Implementación pendiente: desarrollar el endpoint para que el estudiante suba y referencie correctamente la evaluación. Podría ser un POST /api/resources/upload seguido de POST /api/study_plans/evaluation/<eval_id>/submit que internamente cree el EvaluationResource. 3. Evaluación de la entrega: Dos caminos posibles: - Manual: El profesor descarga el archivo, lo califica offline, y luego registra la calificación en la plataforma. Nuevamente, esto implicaría introducir la nota manual (posiblemente a través de la misma interfaz de evaluación manual mencionada antes). El sistema debería dar opción de colocar una calificación y feedback por cada entrega. Esa nota terminaría almacenada (preferiblemente como ContentResult, indicando content_id = linked_quiz_id si existiera o de alguna manera asociada a la evaluación). - Automática (con IA): Aquí entra el módulo de revisión automática de exámenes propuesto. Con la entrega subida (por ejemplo, imagen de examen escrito), se podría invocar un servicio de IA que corrija automáticamente según criterios.
Revisión Automática de Exámenes (Visión e IA): Es una funcionalidad nueva solicitada que no existe aún en el sistema, pero aprovecharemos la infraestructura de recursos y evaluaciones para integrarla: - Entrada: una imagen (foto del examen escrito) o un PDF/Documento de texto que el alumno sube como entrega. - Rubrica/Criterios: el profesor debe haber definido cómo calificar. Puede ser a través de criterios estructurados (el campo criteria de Evaluation podría albergar una lista de ítems con puntajes, e.j. [{"aspect": "Ortografía", "deduction_per_error": 0.5}, ...]). O podría proporcionar un documento con las respuestas correctas o instrucciones (eso se maneja ya como un Resource de apoyo). - Proceso IA: Un servicio de visión artificial debe: 1. Extraer el texto de la imagen (OCR). Esto se guardaría quizá como parte del Resource (añadir Resource.metadata.text_extracted tras procesamiento). 2. Evaluar ese texto vs las respuestas esperadas/rúbrica. Aquí se aplicaría un modelo de lenguaje (ej. GPT-4) con un prompt que incluya: el texto del alumno, los criterios de evaluación (ya sea en lenguaje natural o estructurado), y que devuelva una calificación y justificación. 3. Devolver un resultado estructurado (p. ej., score 0–1 y feedback detallado por criterio). - Almacenamiento resultado: Una vez la IA genere la evaluación, el sistema puede automáticamente crear un ContentResult con score igual a la calificación propuesta y feedback con la explicación de la IA, para esa entrega. De nuevo, necesitamos asociarlo: posiblemente aquí content_id no puede ser el ID del Resource (no conceptuado como “contenido educativo”), sino podríamos usar el mismo evaluation_id como content_id hackeando un poco. Quizá la salida más limpia es: crear un TopicContent especial de tipo “exam” o “project” que represente la evaluación en sí, y usar su ID para ContentResult. De hecho, en ContentTypes hay categorías evaluation incluyendo EXAM, PROJECT, RUBRIC[38], lo que sugiere la posibilidad de manejar evaluaciones como contenidos unificados. Si al crear una Evaluation en el módulo, paralelamente se crea un TopicContent de tipo exam (o project), podríamos vincularlos. Esto no está implementado aún, pero es una idea para reutilizar la misma maquinaria de ContentResult.
•	Intervención del profesor: El sistema automático puede asignar nota, pero se debe permitir al profesor revisarla. Se sugiere:
•	Marcar estos ContentResult auto-generados como “pendientes de confirmación”. Por ejemplo, en ContentResult.metrics incluir {"auto_grade": True} y quizás un flag de aprobado.
•	Notificar al profesor que “La entrega de X ha sido evaluada automáticamente con Y puntos. ¿Confirma esta calificación o desea ajustarla?”. El profesor podría entonces editar la puntuación o feedback. Al confirmar, el ContentResult se actualiza (o se crea uno nuevo final, pero mejor actualizar el existente para historial).
•	Si el profesor no está de acuerdo con la AI, puede sobreescribir la nota o incluso optar por calificar manualmente desde cero.
•	Evaluar código/Programación: Mencionado como posible extensión: permitir que si el alumno sube código (ej. un archivo .py o .html), el sistema pueda ejecutarlo en un sandbox y verificar su salida. Esto se podría integrar en la lógica de IA: si la rúbrica indica un caso de prueba con entrada X y salida Y, el sistema podría correr el código con X y comparar con Y, generando un veredicto. Esto es complejo pero factible con contenedores seguros o servicios especializados (no es prioridad inmediata, se puede dejar como futuro módulo específico de programación). Por ahora basta con mencionarlo como potencial.
Asociar varias actividades a una Evaluación: El usuario sugirió que una Evaluación podría componerse de varios contenidos (ej. dos quizzes de distintos temas). Actualmente, la base de datos no relaciona una Evaluation con múltiples content_ids (solo uno en linked_quiz_id). Si se quisiera soportar eso, se podría: - Extender Evaluation para tener linked_content_ids: List[str] en vez de un solo linked_quiz_id. Pero esto complica cómo mapear puntuación (¿promedio de ambos quizzes? ¿suma?). - Alternativamente, manejarlo externamente: crear una Evaluación “Quiz Módulo Completo” sin linked_quiz, pero cuyo criterio diga “promedio de Quiz1 y Quiz2”. Luego, al calcular la nota, la lógica sería: buscar ambos ContentResults y promediar. Esto requeriría lógica custom, no muy complicado de implementar en el servicio de evaluaciones, pero sí de presentar en UI.
Recomendación: Inicialmente, mantener la simplicidad: una Evaluación formal = o bien un quiz, o bien un proyecto/entrega, o manual. Si en algún caso se requiere abarcar varios quizzes, se puede indicar que se cree una única Evaluación y un único quiz englobando todo el temario (por ejemplo, en lugar de dos quizzes separados en dos temas, hacer uno combinando preguntas de ambos – esto puede lograrse en la generación de contenido del profesor si así lo decide). Así evitamos relaciones N a N. En caso imprescindible, documentar que varias evaluaciones pueden tener ponderaciones que luego el profesor combine offline.
Necesidades de Implementación (Resumen): - Frontend Profesor (Generación de contenidos): Terminar la UI para configurar Evaluaciones: - Permitir seleccionar si la evaluación será un Quiz automático (mostrar dropdown de quizzes disponibles del módulo para vincular, marcar use_quiz_score), - o una Entrega (requires_submission, opcionalmente subir archivos de enunciado/rúbrica en ese mismo formulario), - o Manual (ninguna de las anteriores; solo pide ponderación). - Ingresar weight (peso porcentual o puntos, se debe definir la escala, quizás manejar todo en % o dejarlo en puntos sumando 100). - Ingresar fecha de entrega (due_date). - Criterios: podría ser un campo de texto estructurado o un pequeño generador de rubrica (no obligatorio). - Frontend Estudiante: - Mostrar evaluaciones pendientes en su vista (por ejemplo en la sección del módulo o un tablero de pendientes). Si es requires_submission, dar botón “Subir entrega”. Si es quiz automático, redirigir al tema/quiz correspondiente (o integrarlo). - Para entregas, permitir subir archivo (reusar componente FileUpload). Tras subir, indicar estado "Entregado". - Mostrar nota obtenida cuando esté disponible, con feedback. Esto se puede hacer leyendo ContentResult o alguna API de resultados de evaluaciones. - Backend: - Endpoints para crear evaluaciones (quizás ya existe en study_plans/routes.py), obtener lista de evaluaciones de un módulo, subir recursos (ya existen en resources), y submit entrega (crear EvaluationResource). - Lógica para cálculo de notas finales del módulo: cuando se desee calcular la nota del módulo para un estudiante, combinar todas sus Evaluations según peso. Esto puede hacerse consultando ContentResults (o entradas manuales) para cada evaluación. Podría implementarse en analytics/services.py o en un nuevo servicio de calificaciones. - Integrar IA de corrección: esto implica posiblemente un microservicio aparte. Como plan mínimo, se puede implementar como una función asíncrona en backend que, al detectar una nueva entrega subida (EvaluationResource submission), encola un job de corrección automática. Ese job usaría alguna API (por ejemplo, OpenAI o Google Vision + PaLM) para procesar la imagen y generar calificación. Los resultados se guardan en ContentResult. Este proceso asíncrono puede indicarse en la UI con un estado "Corrigiendo...". Necesitaríamos una cola de tareas (tal vez usar Celery, o aprovechar el modelo VirtualGenerationTask adaptado). Dado que esta característica es avanzada, se puede planificar para una fase posterior de desarrollo (ver Plan de Implementación).
En resumen, el sistema de evaluaciones tiene bases sólidas (modelos y algunas funciones), pero requiere trabajo de integración y UI para hacerlo funcional: - Completar la gestión de notas automáticas via ContentResult, - Permitir la calificación manual en la plataforma, - Manejar entregas (upload, almacenamiento, corrección manual/automática), - Y garantizar que al completar >80% de un tema se llame a POST /api/virtual/trigger-next-topic para desbloquear el siguiente (esto ya está implementado en backend: verifica progreso y genera próximos temas[39][40], solo asegurarse que el frontend lo invoque correctamente cuando corresponda).
5. Generación Progresiva de Módulos Virtuales (Cola de Creación)
El sistema fue diseñado para crear los temas virtuales gradualmente, manteniendo siempre un par de temas “por delante” listos para el estudiante. Revisemos la implementación actual:
•	Creación inicial del Módulo Virtual: Cuando un estudiante accede por primera vez a un módulo que tiene todos sus temas publicados, el backend crea un VirtualModule y utiliza _generate_virtual_topics_fast para generar un lote inicial de temas[41][42]. Actualmente el código está configurado con initial_batch_size = 2[43][42], es decir, solo genera los primeros 2 temas virtuales del módulo. Ambos se marcan como active y locked=False (desbloqueados)[44]. Diferencia con lo esperado: La descripción del usuario sugería generar 3 temas (1° accesible de inmediato y 2 en cola). Con la implementación actual, si solo hay 2 temas generados y ambos desbloqueados, el estudiante podría hipotéticamente saltar al segundo sin terminar el primero. Sería preferible ajustarlo a la visión original: generar 3 temas iniciales, pero quizá marcar el 2° y 3° como locked=true (bloqueados) para que el alumno no avance fuera de orden. Sin embargo, el enfoque adoptado parece ser desbloquear ambos para dar más libertad. Esto requiere confirmar con los requerimientos pedagógicos: si se desea que el estudiante siga secuencialmente, habría que imponer locked en todos excepto el actual.
•	Desencadenar siguientes temas: Está implementado en trigger_next_topic_generation. Cuando el frontend notifica que el progreso de un tema ≥ 80%, el backend:
•	Busca la posición del tema actual en la lista de originales del módulo.
•	Ve qué temas originales aún no tienen VirtualTopic generado[45].
•	Genera hasta 2 nuevos temas siguientes que no existían[40][46]. Estos se crean como status: active y locked: False[47] para acceso inmediato.
•	Marca el tema actual como completed si llegó a 100%[31].
•	Responde indicando si quedan más temas por generar (has_next)[48].
Esto confirma la política de "mantener dos por delante": tras completar ~80% del Tema 1, generará Temas 2 y 3 (si no existían). Tras el Tema 2, generará 4 (y 5 quizá, dependiendo de cuántos quedaron). Siempre asegura que al menos el siguiente inmediato esté disponible. Cita: “Generar hasta 2 temas por delante”[49]. En otras palabras, al terminar cada tema, habrá otros dos listos (excepto al final del módulo).
•	Virtualización de módulos siguientes: El código sugiere que la generación se hace módulo por módulo. No vimos explícitamente que prepare el siguiente módulo antes de acabar el actual. Sin embargo, es razonable pensar que al terminar el último tema del módulo A, se podría invocar la creación del VirtualModule del módulo B. En el momento que el backend libera nuevos módulos (vía /trigger-next-topic quizás?), habría que generar el módulo virtual siguiente. Esto puede lograrse detectando que no hay más temas en el módulo actual (has_next = False y quizá se completó el módulo), entonces automáticamente crear el VirtualModule del siguiente módulo del plan. Esta lógica no la observamos en el código mostrado. Podría requerir implementación: quizá ampliar trigger-next-topic para que si un módulo terminó y existe otro módulo en el plan que el estudiante aún no tiene, inicie la generación de ese. Alternativamente, cuando el estudiante entre al siguiente módulo por primera vez, se llame a la función de generación inicial para ese módulo. Recomendación: Implementar un mecanismo similar para anticipar módulos: cuando queden 2 temas para terminar el módulo X, empezar a crear el módulo X+1 en background. Esto evitaría esperas largas al cambiar de módulo. Se podría incluir en trigger_next_topic_generation una detección: si has_next == False (no hay más temas en el módulo actual) pero existe un siguiente módulo en el plan de estudio, entonces invocar VirtualModuleService.generate_module(student_id, next_module_id) para que arranque con ese módulo (o al menos sus primeros temas). Esto alinearía con la idea de 2 módulos por delante mencionada.
•	Manejo de errores y reintentos: El usuario indicó que si un modelo de IA falla generando contenido, se reintente con otro modelo. Contexto: Esto tiene más que ver con la generación de contenidos originales (del profesor) que con la virtualización. Pero en virtualización podría haber fallos (p.ej., fallo de conexión a la IA de personalización? Aunque la personalización es en frontend, así que no hay fallo crítico salvo no reemplazar marcadores). En la creación de VirtualTopics, el código ya atrapa excepciones y continúa con los siguientes (ej: si falla un tema, lo loguea y sigue[50], o si falla la generación en trigger, igualmente continúa con el siguiente tema en loop[51]). Esto es correcto: un fallo al generar un tema no debe detener todo el proceso. Deberíamos luego ofrecer un mecanismo de reintento manual (por ejemplo, un botón “Reintentar generar contenido” si un tema virtual quedó incompleto por algún error).
•	Persistencia de la cola de tareas: Hay un modelo VirtualGenerationTask definido[52], pensado para una cola persistente de tareas. No obstante, en la implementación actual parece no usarse (el código que vimos genera directamente los temas sin encolar, de forma síncrona). En un despliegue escalable, convendría usar esa cola: crear tareas “generate_module” o “generate_topic” que un worker aparte consuma, marcando status. Dado que la generación está siendo rápida y on-demand, quizás lo dejaron para futuro. Recomendación: Si el rendimiento comienza a ser un problema o se integran modelos IA más lentos, activar el uso de VirtualGenerationTask: insertar tareas y tener un proceso (o función serverless) que las procese, actualizando VirtualModule.generation_status y generation_progress. Esto también ayuda para mostrar en frontend una barra de progreso mientras se genera.
•	Feedback en frontend durante generación: El usuario quiere que si recarga la página o navega fuera, la generación continúe y al volver pueda ver el estado (mediante notificaciones). Para lograrlo:
•	La generación en backend ya continúa independiente del frontend (no depende de la sesión del usuario, una vez iniciada).
•	Lo que falta es comunicarle al usuario el progreso. Podríamos aprovechar los campos generation_status y generation_progress de VirtualModule[53][54]. Por ejemplo, al iniciar un módulo virtual, establecer generation_status="generating", generation_progress=50%[55], y al terminar poner "completed" 100%[56]. El frontend podría hacer polling o usar WebSocket para recibir actualizaciones. Una solución simple: cada vez que el usuario visita la pantalla del módulo virtual, llamar a un endpoint que devuelva el estado (ya existe get_module_details[57] que incluye generation_status y progress). Con eso se puede decidir si mostrar un mensaje "Generando contenido..." con un spinner. Para persistir notificaciones tras refresh, se podrían usar componentes de notificación global con estado en algún contexto que sobreviva, o re-chequear la lista de contenidos generados vs total esperados.
•	Dado que ahora la generación es bastante rápida (solo copiar contenidos existentes, la parte tardada es realmente la IA secuenciando y personalizando pero eso es en frontend), quizás no sea crítico. Sin embargo, cuando integremos IA más potente en personalización de tipo de contenido, la generación de un VirtualModule podría tardar más (si, por ejemplo, en el futuro se genera contenido adicional personalizado). Estar listos para manejarlo es bueno.
Conclusión: La mecánica de generación progresiva está implementada correctamente en cuanto a temas por módulo[40][46]. Se sugiere pequeños ajustes: - Aumentar a 3 el lote inicial o decidir si 2 es suficiente (considerando la usabilidad: 3 iniciales darían margen si hay un pequeño retraso en generar el 3° tras el 80% del 1°). - Decidir la política de locked: actualmente los nuevos temas salen locked=False por código[58], lo cual efectivamente permite al alumno abrirlos sin esperar. Si se desea forzar secuencia, habría que cambiarlos a locked=True excepto el inmediato siguiente. Una solución intermedia: marcar desbloqueado solo el siguiente tema, y los demás bloqueados. Esto implicaría después de generar dos, bloquear el segundo en la lista hasta que el anterior esté completo. Podríamos manejarlo en frontend (no listarlos clicables) o modificando la propiedad en BD. Por simplicidad, podríamos: generar con locked True, menos el caso en que sea el próximo inmediato. - Implementar pre-generación del siguiente módulo (fase futura, mejora de UX). - Mejorar notificación de progreso de generación usando generation_progress y notificaciones persistentes.
6. Integración de Múltiples Modelos de IA para Generación de Contenido
En la generación de contenidos originales (vista del profesor, Topic Generation), se busca acelerar y robustecer el proceso usando múltiples modelos en paralelo. Actualmente: - Hay soporte en frontend para múltiples modelos de IA. Vemos, por ejemplo, ejemplos de configuración donde se alternan modelos como o3_mini_open_router, gemini_2_5_flash, gemini_2_5_flash_open_router, gemini_2_5_pro[59][60]. Esto sugiere que ya se integraron distintas APIs (OpenRouter, Google PaLM Gemini, etc.) y se puede especificar lista de modelos y patrón de uso (alternar, aleatorio, secuencial)[61]. Sin embargo, esto parece más orientado a alternar dentro de la generación de un mismo recurso (ejemplo: generar distintas diapositivas con distintos modelos en secuencia) que a la paralelización real. - La petición del usuario es: “no generar uno por uno, sino tres a la vez”. Es decir, si un tema tiene que generar Texto, Diagrama, Quiz (por ejemplo), usar 3 hilos/modelos para hacerlo concurrentemente, reduciendo el tiempo total casi a un tercio.
Estado Actual: La generación de cada tipo de contenido seguramente ocurre secuencialmente actualmente. Es probable que el backend o frontend llame a un endpoint por cada tipo en orden (primero texto teórico, luego quiz, etc.). No vimos el código específico, pero dado que mencionan tardanzas, infiero que no hay concurrencia implementada todavía.
Implementación Propuesta: - Asignación de modelos por tipo de contenido: Por ejemplo, - utilizar el modelo más potente (Gemini 2.5 Pro) para la tarea más compleja o crucial (quizás generar el contenido teórico extenso, o las diapositivas detalladas), - usar modelos rápidos (Gemini Flash, o modelos OpenRouter) para contenidos más ligeros (diagramas sencillos, glosarios, quizzes de pocas preguntas). - Ejecución en paralelo: Si usamos Next.js en frontend, podríamos disparar varias peticiones de generación simultáneamente mediante Promise.all. O bien, tener un endpoint en backend que internamente lance sub-tareas en paralelo. Dado que los conectores de IA probablemente sean llamados desde el frontend (por la presencia de esos servicios en src/services/AI/... del frontend), lo más directo es manejarlo en frontend: - Al hacer clic en “Generar contenido” de un tema, en vez de un ciclo síncrono, iniciar 3 promesas: por ejemplo generateContent(type=slides), generateContent(type=diagram), generateContent(type=quiz). - Monitorear las promesas: conforme se resuelva cada una, actualizar la UI (p.ej. una lista de tareas de generación con estado: “Texto ✓, Diagrama ✓, Quiz en proceso…”). - Si alguna falla (error API del modelo), capturarla. El usuario desea: “si falla un modelo generando un contenido, asignarlo a otro modelo y reintentar”. Para esto, habría que tener un fallback: por ejemplo, si la generación de diagrama con OpenRouter falla, reintentar la misma petición con Gemini Flash. Podríamos configurar un orden de preferencia para cada tipo: - Texto: 1° Gemini Pro, fallback GPT-4 (si estuviera), etc. - Diagrama: 1° Gemini Flash, fallback OpenRouter image model. - Quiz: 1° Gemini Pro (por comprensión), fallback GPT-4. - Implementar reintento: tras un error, loguearlo y llamar de nuevo con el siguiente modelo disponible. No conviene reintentar inmediatamente con el mismo modelo (si falló posiblemente es por sobrecarga o formato), por eso saltamos a otro. En la siguiente generación de otro contenido, se puede volver a intentar el modelo fallido (como piden: no desecharlo para siempre, sino probarlo más tarde). - También conviene establecer un pequeño delay o manejo de límite de llamadas simultáneas al mismo servicio para no saturar.
•	Seguimiento tras refresh: Sería deseable que si el profesor recarga la página durante la generación, al volver pueda ver qué contenidos ya se generaron y cuáles están pendientes. Para ello:
•	Hacer que la generación de contenido de profesor se apoye más en backend. Por ejemplo, cuando inicia la generación de un contenido, crear un registro (en BD o al menos en memoria del servidor) del estado. Quizá un modelo ContentGenerationTask similar a VirtualGenerationTask.
•	Alternativamente, dado que es menos crítico (el profesor puede simplemente volver a pedir generar el que falte), se puede posponer. Pero como mejora: si se guardan los contenidos generados parcialmente, la página puede consultarlos. De hecho, los contenidos generados se guardan inmediatamente en TopicContent de la base (draft o active según el caso). Por tanto, tras refresh, el frontend puede consultar la lista de topic_contents del tema: los que ya estén ahí indicarán qué ya fue generado. Los faltantes (tipos que no aparecen) se podrían regenerar.
•	Implementar notificaciones persistentes similar a la idea en módulos virtuales: un contexto global que retenga “Generando X e Y…” aunque se navegue.
Beneficios: Con concurrencia, si generar texto tarda 20s, diagrama 15s y quiz 25s, actualmente tardaría ~60s secuencialmente; en paralelo tardaría ~25s (lo que más dure). Además, la redundancia de modelos aumenta tasa de éxito: si uno falla, otro sigue. El ejemplo de alternar entre 3 modelos en la generación de slides[62][59] muestra que ya contemplaron aprovechar varios modelos. Podemos extender esa idea a diferentes contenidos simultáneos.
Precauciones: Usar 3 modelos concurrentes consumirá más créditos de IA simultáneamente. Debemos asegurarnos de tener control (p.ej. no lanzar 3 GPT-4 a la vez si costea mucho; por eso sugieren modelos más económicos para algunas tareas). También, manejar condiciones de error (si la API key de un modelo expira, etc., probar fallback automáticamente).
Resumen de tareas: - Configurar en código qué modelos usar por defecto para cada tipo de generación (quiz, summary/texto, slides, diagram, etc.). - Modificar la función de generación para lanzar llamadas concurrentes. Posiblemente implementar en unifiedContentGenerator una lógica donde, dado un array de tipos, asigne cada uno a un modelo y llame. - Implementar reintentos y fallback de modelos. Se puede almacenar un pequeño estado: por ejemplo, un objeto { modelFailures: { "gemini_flash": lastFailureTimestamp, ...} } para no reutilizar inmediatamente uno que falló, pero permitirlo más adelante. - UI: mostrar una indicación de progreso para cada tipo. Podría ser una lista de checkboxes o un spinner global con texto cambiante (“Generando Diagrama…”). Ya utilizan toast notifications (posiblemente los toasts que mencionan). Deberían persistir tras navegación: quizás guardando en localStorage el estado de “generación en curso” y restaurándolo onLoad.
7. Nuevos Tipos de Contenido Educativo a Integrar
El sistema ya contempla en su enumeración muchos tipos de contenido (teóricos, visuales, multimedia, interactivos, evaluativos)[63][64]. Sin embargo, no todos están implementados en generación. El usuario desea ampliar la variedad de recursos pedagógicos disponibles para personalizar la enseñanza.
Tipos actuales y su estado de implementación: - Teóricos: Texto explicativo (método Feynman), resúmenes, glosarios, ejemplos, documentos PDF. Implementación: El generador de contenido teórico principal (método Feynman) ya existe. No está claro si glosario y ejemplos se están generando automáticamente; es posible que no. Sería útil añadir: glosario de términos clave del tema, preguntas guiadas (como comprobación de lectura), etc., que son tipos listados[65]. - Visuales: Diagramas, infografías, mapas mentales, líneas de tiempo, ilustraciones. Implementación: Actualmente hay un generador de diagrama funcional (basado en DALL-E o similar). Se menciona que la generación de simulaciones/juegos es un proceso aparte manual (sección 9). Infografías, mindmaps, etc., no parecen implementados todavía. Podrían generarse usando IA (aunque la creación de un mindmap automático es compleja; quizás se puede generar texto estructurado y renderizarlo en front). - Multimedia: Video, audio, animaciones. Implementación: No se ha integrado generación de video/animación (eso requeriría herramientas IA adicionales, muy costosas). Probablemente videos se manejarían como recursos externos (enlaces a YouTube) más que generarlos. Audio (narraciones) se podría generar con texto a voz del contenido teórico, lo cual es viable. - Interactivos: Juegos, simulaciones, laboratorios virtuales, AR (realidad aumentada), ejercicios interactivos, flashcards. Implementación: Un quiz simple está unificado vía TopicContent (las preguntas se generan como JSON en interactive_data). Un juego completo es más complejo – por eso hay una sección de plantillas de juego (ver sección 9). Flashcards podrían generarse a partir del contenido (por ejemplo extrayendo pregunta->respuesta breves). Esto sería relativamente fácil de implementar: tras generar texto o diapositivas, tomar hechos importantes y formar tarjetas. - Evaluativos: Quiz, exámenes (preguntas abiertas), proyectos, rúbricas. Implementación: Quiz está, exámenes abiertos se pueden simular con la parte de entregas manuales/automáticas, rúbricas puede que no se generen pero se pueden definir manualmente.
Prioridades de integración: - Gemini Live (Interrogador conversacional): Ya se ha iniciado desarrollo de este tipo. Es un tipo interactivo evaluativo en el que el estudiante conversa con una IA sobre el tema, y al final la IA le da un puntaje de entendimiento. El código muestra un GeminiLiveInterrogator que usa la API de Google GenAI con modalidad de audio y texto[66][67]. Para integrarlo completamente: - Crear un TopicContent de tipo conversation o similar, que al iniciarse en frontend lanza el componente de Interrogator. - Este componente ya transcribe audio en vivo y envía preguntas, recibiendo audio de respuesta de la IA (voz Zephyr)[68][69]. - Debemos decidir cómo calificar al final. Podríamos hacer que la IA misma, tras cierta cantidad de preguntas, diga un resumen (“Veo que dominas bien el tema con un 8/10”). Alternativamente, contar número de respuestas correctas (difícil sin supervisión). Dado que es experimental, quizá la IA generará un feedback cualitativo más que una nota numérica. En cualquier caso, podríamos mapearlo a un ContentResult: la sesión genera algún score implícito o se puede pedir al modelo que devuelva un porcentaje de comprensión. - Integración con UI: Podría presentarse como un “modo examen oral”. Este tipo diversifica la evaluación y entrena al estudiante en explicación verbal, lo cual es excelente. - Estado: En código existe InterrogatorComponent.tsx probablemente para la UI. Falta integrarlo en la secuencia de contenidos (tal vez al final de un tema como “Conversa con SapiensIA sobre este tema”).
•	Flashcards (tarjetas de memoria): No se mencionó explícitamente en la conversación, pero dado que se listan en ContentTypes y la repetición espaciada es muy útil, podría ser un tipo a sumar. Generarlas: la IA puede crear ~5 flashcards Q&A del tema. Frontend: mostrar tarjeta con pregunta, flip para respuesta, y permitir al estudiante marcar si la recordó bien o no, para futuras revisiones (espaciadas). Esto puede almacenarse en ContentResult metrics (e.g., cuántas recordó bien).
•	Ejercicios de rellenar espacios o opción múltiple integrados en texto: Podría haber contenido interactivo ligero incrustado en la teoría (p.ej., después de un párrafo, una pregunta de opción múltiple para reforzar). Esto se logra ya en parte con los quizzes, pero se han tratado como contenido aparte. Intercalar mini-preguntas en el propio texto podría ser interesante pedagógicamente (eso sí, la orquestación debería mantenerlos juntos al texto correspondiente).
•	Contenido colaborativo o peer-review: Dado que hay ContentTypes para peer_review y portfolio, quizás a futuro se piense en actividades entre alumnos (no prioritario ahora).
•	Recursos externos y multimedia reales: Además de generar contenido, la plataforma podría recomendar/videos externos. Actualmente se pueden adjuntar Resource y web_resources en TopicContent[70][71], pero la generación no los llena automáticamente. Podríamos integrar una búsqueda (por ejemplo, usar la API de YouTube para obtener un video relevante del tema y adjuntarlo). Esto enriquecería sin costar IA. Un ejemplo: tras generar contenido teórico, hacer una búsqueda con palabras clave del tema para traer enlaces o videos educativos, guardarlos en web_resources. Así, en el módulo virtual aparecerían como contenido adicional. De hecho, en get_topic_contents vemos que integra topic_resources legacy como legacy_resource en la lista[72][73], lo que significa que recursos subidos manualmente por el profesor se mostrarán. Extender eso para web_resources (links) también sería útil.
Sandbox y visores de contenido: Es importante que por cada tipo nuevo, el frontend tenga un “visor” correcto: - Diapositivas: render HTML con estilos. - Diagramas/Imágenes: mostrar la imagen generada (actualmente se guarda la URL supongo). - Juegos/Simulaciones: quizás un iframe o componente personalizado. - Conversación (Gemini Live): un componente con audio y texto. - Flashcards: un pequeño widget interactivo. - etc.
Ya se menciona la necesidad de que “los visores y sandboxes no tengan fugas de estilo ni errores”. Esto significa encapsular bien cada contenido (ej. si se inserta HTML del modelo, asegurar que no rompa estilos globales; quizás usando <iframe sandbox> o sanitizando CSS). Probablemente habrá que probar cada tipo en distintos tamaños de pantalla (responsividad).
Resumen: Expandir tipos de contenido aumenta la personalización pero requiere: - Generación IA o lógica para cada tipo (posiblemente uno por uno en distintas fases, priorizando los de mayor valor educativo: flashcards, preguntas conversacionales, ejercicios prácticos). - Soporte de frontend para mostrarlos e interactuar correctamente. - Guardar resultados cuando corresponda (ej. flashcards no tienen “nota”, pero se podría guardar el uso para refuerzo).
Recomendamos planificar la integración de nuevos tipos por fases, iniciando con los de evaluación/repaso (flashcards, conversación) que complementan el aprendizaje, y luego añadir más formatos informativos (infografías, etc.) según recursos disponibles.
8. Actualizaciones de Contenido en Curso (Sincronización de Cambios)
Un aspecto crítico: ¿Qué ocurre si el profesor modifica el plan de estudio (agrega/elimina contenidos o temas) cuando estudiantes ya tienen módulos virtuales en progreso? Debemos asegurar que los cambios se reflejen sin incoherencias, manteniendo a cada estudiante actualizado.
El backend ya tiene una función synchronize_module_content(virtual_module_id)[74] que: - Compara los temas originales publicados vs los VirtualTopics existentes[75][76]. - Añade nuevos VirtualTopics para cualquier tema original nuevo[77][78] (los crea bloqueados por defecto[79] para que el estudiante no los vea hasta que llegue el momento). - Luego, para cada tema virtual existente, compara los TopicContent originales vs los VirtualTopicContent presentes[80][81]: - Crea VirtualTopicContent para cualquier contenido original nuevo que el estudiante no tenga[18][19]. Los marca con personalization_data.sync_generated=True y fecha[19]. Importante: pone adapted_content=None y copia el contenido original tal cual (no lo personaliza en ese momento)[19][82]. Esto significa que si el profe añadió un contenido nuevo, el estudiante lo obtendrá posiblemente sin adaptación a su perfil en el momento de sincronizar, a menos que implementemos que en ese instante se aplique personalización (podría invocarse _generate_topic_contents_for_sync que ya hace la inserción; quizá convendría mejorarla para personalizar también). - Elimina/archiva contenidos virtuales si el original fue eliminado (set status="archived" en los VirtualTopicContent huérfanos)[83]. - Nota: No implementa actualización de contenido modificado – está simplificado como que si un contenido cambió internamente pero mantiene el mismo ID, no se sincroniza (lo menciona: “por ahora no se detectan cambios internos”[84]). Esto es delicado: si el profesor edita el texto de un contenido, el estudiante virtual no verá ese cambio, ya que su VirtualTopicContent seguirá con el viejo content. Solución: implementar detección de cambios, quizá comparando TopicContent.updated_at con VirtualTopicContent.created_at y, si hay gran diferencia, actualizarlo. O llevar un hash de contenido en personalization_data para comparación. Este es un TODO.
•	Registra la actualización en virtual_module.updates con un reporte de added/removed[85].
Esta función parece estar preparada pero ¿cuándo se llama? Posiblemente: - Manualmente por un admin, o planeado para ejecutarse cuando el estudiante abra el módulo (por ejemplo, al entrar de nuevo a un módulo virtual, podría dispararse sincronización). - Sería prudente llamarla al desbloquear un tema nuevo: antes de generar temas nuevos, sincronizar contenidos existentes. O simplemente en un horario (ej. un cron cada noche sincroniza todo, pero eso puede ser tardío). - Recomendación: Llamar synchronize_module_content dentro de trigger_next_topic_generation antes o después de generar nuevos temas, para asegurarse que lo ya existente esté al día. Mejor aún, exponer un endpoint POST /api/virtual/module/<id>/sync para que el frontend pueda forzar la sincronización (por ejemplo, un botón “Actualizar contenido” para el estudiante si sospecha cambios).
Incorporar un contenido nuevo sobre la marcha: El usuario preguntó: si se agrega un nuevo tipo de contenido a un tema que el alumno ya cursó parcialmente, ¿qué hacer? Según la lógica: - synchronize_module_content añadirá ese VirtualTopicContent al estudiante. Dado que en get_topic_contents los contenidos se ordenan por created_at[86][87], ese nuevo contenido probablemente aparecerá al final de la lista en la UI (ya que tiene fecha reciente). - Esto puede estar bien (lo muestra como contenido adicional nuevo). Podríamos marcarlo visiblemente como "Nuevo contenido añadido". - Alternativamente, re-correr useTopicOrchestrator para reordenar todo incluyendo el nuevo. Pero reorganizar drásticamente un tema que el alumno ya venía leyendo puede ser confuso. Quizá mejor no reordenar retroactivamente, sino anexar al final o en una sección “Recursos adicionales”. - Actualmente el código sí mezcla los contenidos con IA al secuenciar, pero dado que source_type de uno nuevo sería “virtual_content” normal, en un refresco de orquestación podría posicionarlo distinto. Sin embargo, useTopicOrchestrator sólo corre en el momento de cargar un tema. Si el estudiante ya tiene la pantalla abierta, habría que notificarle. - Posible solución: si agregamos contenido a un tema ya visible para el alumno, podríamos enviar un evento (WebSocket) “nuevo contenido disponible” para que la UI lo integre dinámicamente.
•	Si un contenido fue eliminado por el profesor: la sincronización lo archiva. Habría que decidir si ocultarlo de la vista del estudiante (posiblemente sí, no tiene caso estudiarlo si el profesor lo quitó). Podríamos filtrar en get_topic_contents para no devolver contenidos con status archived. Actualmente no lo hace explícitamente, pero podría añadirse filtro {"status": {"$ne": "archived"}} en la query de virtual_topic_contents[88]. Eso sería recomendable.
Desbloqueo de temas y contenidos: Ya cubrimos en progresión: cuando ≥80% completado, se desbloquea el siguiente tema (vía trigger endpoint). Los contenidos en un tema no se bloquean individualmente, salvo que se quiera imponer secuencia interna (no está planteado; se asume el estudiante puede navegar dentro del tema en cualquier orden). Bastará con controlar temas completos.
Progreso y marcado de lectura: Cada VirtualTopicContent tiene completion_status y porcentaje[89]. El frontend debe marcar como completado cuando el estudiante finaliza la interacción (por ejemplo, al llegar al final del texto, o al terminar un quiz). Esto puede hacerse mediante: - Actualizar VirtualTopicContent.interaction_tracking via API (quizás un PUT /api/virtual/content/<id>/track). - O generar un ContentResult como se habló, lo cual indirectamente indica completado.
De cualquier modo, asegurarse de actualizar VirtualTopic.progress. Esto probablemente se calcula on-the-fly sumando completados/total cada vez que se pide (o se actualiza en BD cada vez que cambia un content). De hecho, en trigger_next_topic_generation se pasa el progress del tema actual como parámetro, asumiendo el front lo calculó y envió[90]. Quizás la app calcula progress = completados/total localmente y lo envía. Esto está bien.
Resumen: La infraestructura de sincronización existe pero hay que: - Invocarla en los momentos adecuados (al entrar a un módulo, o al terminar un tema). - Mejorar para detectar cambios de contenido (no solo adds/removes). - Decidir cómo presentar nuevos contenidos añadidos: preferiblemente sin reordenar radicalmente lo ya visto. Lo más simple: anexarlos al final como “Contenido adicional publicado el <fecha>”. - Ocultar contenidos eliminados limpiamente de la vista del alumno. - Comunicarle de alguna manera al estudiante que su material se actualizó (una pequeña notificación tipo “Tu profesor ha añadido un nuevo recurso al tema X”).
9. Generador de Juegos/Simulaciones y Marketplace de Plantillas
Se menciona que la creación de juegos y simulaciones se maneja aparte porque es más compleja. Existe incluso una sección separada en la interfaz del profesor para esto. La idea es permitir a los profesores generar plantillas de juegos que luego puedan reutilizarse en distintos contextos.
Estado Actual: - Probablemente hay un módulo en frontend (quizá bajo “Generación de Juegos”) donde el profesor describe un juego y la IA genera código (por ejemplo, un mini-juego estilo Angry Birds con cierto tema matemático). Esos juegos, una vez generados, seguramente se guardan como un recurso embebido (posiblemente un archivo HTML/JS en la base de datos o como gist). - No disponemos de detalles precisos en el código dado, pero el usuario quiere que esas plantillas se almacenen para reutilizar. Plan: Cuando un juego se genera, guardarlo en una colección de GameTemplates con metadatos (tipo de juego, habilidades que ejercita, temática). Luego: - Ofrecer un Marketplace de plantillas de juegos/simulaciones para que otros profesores puedan importarlas a sus cursos. Por ejemplo, un profesor crea un juego de sumas -> otro profesor de física podría reutilizar la mecánica adaptando las preguntas. - Permitir al generador de contenidos normal (Topic Generation) usar plantillas existentes en lugar de siempre generar de cero: es más eficiente y seguro. Por ej., si hay una plantilla “Juego de vocabulario tipo ahorcado”, al generar contenidos para un tema de idiomas se podría instanciar esa plantilla con las palabras del glosario en vez de generar un juego nuevo. - Esto implicaría: - Una interfaz de catálogo: lista de plantillas con previsualización, etiquetas (ej. “matemáticas”, “rompecabezas”, “memoria”). - Botón "Usar esta plantilla en mi tema": que pida quizá parámetros (p.ej., ingrese las preguntas del juego o los datos específicos) y cree un TopicContent de tipo game en su tema con esa plantilla referenciada. - La plantilla podría ser un código HTML/JS con placeholders. O podría ser una descripción formal que la IA luego rellena. Lo más directo: almacenar el código final generado como base64 o en un repositorio, y al usarlo, solo cambiar ciertos assets.
Desafío: Variabilidad. No todos los juegos son adaptables a todo contenido. Habría que diseñar plantillas genéricas (quiz, memoria, ahorcado, arrastrar y soltar, etc.) que se puedan llenar con contenido de cualquier tema.
El usuario insinúa que la sección de generación de juegos actual crea la base (plantilla) que se guarda, y luego desde Topic Generation (generación de contenidos del tema) esa plantilla se puede reutilizar pasando solo cambios menores (ej. cambiar preguntas de suma a resta).
Implementación: - Backend: Crear modelos para GameTemplate (campos: nombre, descripción, author_id, perhaps a reference to a Resource storing the code or a URL). - Guardar plantilla al generarla: Cuando un profesor genera un juego en la sección especial, además de guardarlo como un Content específico de su tema, guardarlo también como plantilla independiente (si está bien logrado). - UI Marketplace: Podría ser simplemente una página con tarjetas de cada plantilla (obtenidas de BD). Los profesores pueden previsualizar (quizá un GIF o capturas), leer descripción y reviews (futuro), y con un botón “Importar”. - Importación: Al importar, podría duplicar el Content del juego en su tema actual con las adaptaciones necesarias. Si son mínimas, tal vez solo cambiar el texto de pregunta o imágenes a las de su tema (la IA podría ayudar: "ajusta este juego de plantilla X al tema Y"). - Compartición: Podría haber opción de publicar plantillas creadas por usuarios al marketplace público (quizá moderado por admins para asegurar calidad y que no contengan código malicioso). - Seguridad: Insertar código de terceros es un riesgo. Deberíamos servir esos juegos en un entorno aislado (iframe sandbox sin permitir esquemas peligrosos). Revisar que los recursos (imágenes, scripts) estén bien encapsulados.
Conclusión: Esta funcionalidad apunta a escalabilidad de contenido. Es compleja pero no urgente para MVP. Se podría posponer a fases finales, tras consolidar los módulos virtuales y evaluaciones. A nivel de código, requiere bastante desarrollo tanto backend (almacenamiento y endpoints) como frontend (interfaces de exploración y adaptación). Lo anotaremos en el plan como fase separada (Marketplace de contenidos/games).
10. Funcionalidad de Lenguas Indígenas y Diccionarios
El sistema incluye un módulo para idiomas indígenas, evidenciando su intención de preservar y enseñar idiomas locales. Hay endpoints para crear traducciones, buscar, listar idiomas, etc. Por ejemplo: - POST /translations para agregar una nueva traducción (requiere campos español, traducción, dialecto, language_pair, etc.)[91][92]. - GET /translations y /search para consultar entradas con filtros (dialecto, verificador, mínimo de verificaciones, etc.)[93][94]. - Se maneja un sistema de verificación comunitaria: campos como min_verificaciones, verificador_id sugieren que varios usuarios pueden verificar una traducción. - GET /languages lista los idiomas indígenas disponibles[95]. - Solo admins pueden añadir un nuevo idioma (POST /language)[96]. - Se puede hacer POST /bulk para cargar muchas traducciones de golpe[97].
Estado Actual: Los servicios (TranslationService, etc.) existen, lo que implica que la base de datos tiene colecciones de translations y languages. Probablemente hay una interfaz (quizás en la cuenta admin o profesor) para gestionar un diccionario bilingüe.
Uso previsto: Posiblemente en cursos de idiomas o materias interculturales, los profesores/alumnos puedan contribuir traducciones de términos. Por ejemplo, español -> náhuatl, con dialecto. Este es un proyecto adicional dentro de la plataforma: - Podría integrarse en la creación de contenido: ej., si en un texto teórico aparece un término marcado como [palabra indígena], se podría hacer clic para ver su traducción al español. - O podría ser un módulo separado de aprende vocabulario indígena.
Faltante: Evaluar qué falta por terminar: - Quizá una UI amistosa para que hablantes nativos verifiquen traducciones (un flujo de crowd-sourcing). - El usuario pide "revísalo simplemente y ve qué hace falta". Pudiera faltar: - Paginación o filtros avanzados en búsquedas. - Autocompletar dialectos, etc. - Asegurar que se registren quién verifica (puede que VerificadorService maneje eso). - UI de aprobación para administradores (p.ej. descartar spam).
Dado que es un módulo de nicho, lo documentaremos pero su finalización quizá tenga menor prioridad que las funcionalidades nucleares (módulos virtuales, evaluaciones). Sin embargo, es importante para los objetivos culturales, así que incluir en plan de alguna fase intermedia.
Recomendación: Completar la interfaz: - Mostrar el diccionario en la plataforma (por ej., una pestaña "Diccionario Indígena" donde se pueda buscar palabras). - Permitir a usuarios registrados sugerir traducciones (ya hay endpoint create). - Permitir votar o marcar "correcto" para incrementar verificaciones. - Eventualmente, integrar estas traducciones a los contenidos: por ejemplo, si un contenido se imparte en una lengua indígena o se quiere incluir glosario bilingüe, tener la base de datos lista es útil.
11. Mejoras de UI/UX Generales y Funcionalidades Complementarias
Además de las funcionalidades pedagógicas, el usuario resaltó varias mejoras transversales:
•	Responsividad y Tema Claro/Oscuro: Ya se incorporó un sistema de theming (posiblemente via Tailwind dark mode classes). Se debe asegurar que todas las pantallas soporten ambos temas sin fallos visuales, y que la responsividad (mobile friendly) esté completa. Quizás algunas vistas de tablas o formularios no se adaptaban bien a pantallas pequeñas; esto hay que probarlo y ajustarlo. Dado que se planea una app móvil separada, es posible que la versión web mobile no requiera todas las funciones, pero debe verse correctamente al menos para consultas rápidas.
•	Eliminar archivos obsoletos: Con el desarrollo ágil, es común que queden componentes no usados, imágenes duplicadas, etc. Hacer una limpieza incrementa claridad y reduce bloat del bundle. Esto es una tarea sencilla: identificar módulos o utils ya reemplazados (por ejemplo, si había un antiguo sistema de quiz ahora unificado, eliminarlo). Se puede hacer antes de lanzar producción.
•	Unificar llamadas API con autenticación: Todas las peticiones a /api/* deben usar fetchWithAuth o su variante (posiblemente axios interceptors) para incluir el token JWT. El usuario recalca revisar que no haya llamadas directas sin token (que fallarían una vez se active auth en backend). Un repaso a los servicios frontend muestra que muchos ya importan fetchWithAuth, pero hay que asegurar que funciones nuevas (quizás en pruebas) no lo omitan. También manejar correctamente respuestas de error globalmente: por ejemplo, si el token expiró, redirigir a login; si un error 500, mostrar toast "Error del servidor"; etc.
•	Protección de rutas según rol: Asegurarse de que la navegación no permita que, por ejemplo, un estudiante acceda a vistas de profesor escribiendo la URL. Debe haber guardas que chequeen el rol del usuario logueado y redirijan. Esto puede ser en el lado cliente (React Router con rutas privadas) o también en backend devolviendo 403 si un estudiante intenta llamar endpoints de profesor. En APIRoute.standard quizás se puede pasar roles permitidos (lo vimos en /language, require admin)[98]. Entonces, ya hay protección backend en muchos endpoints. Solo verificar consistencia.
•	Manejo de errores mejorado: Actualmente los errores pueden que solo hagan console.log. Sería bueno brindar feedback al usuario (toasts rojos, etc.). Implementar un componente global de ErrorBoundary para capturar excepciones no manejadas en React y mostrar mensaje amigable.
•	Landing Page pública: Cambiar la ruta raíz / para que no redirija al dashboard por defecto sino a una página de marketing con información de la plataforma, planes de suscripción, etc. Esto implica:
•	Crear un componente Landing (posiblemente ya hay un diseño en figma).
•	Incluir en él botones "Iniciar sesión" y "Registrarse".
•	Mostrar los planes: Institucional, Profesor Individual, Estudiante Individual, con sus precios y características.
•	Este cambio requiere configurar la ruta inicial para usuarios no autenticados. Si ya hay un Router implementado, definir que if !isLoggedIn show <LandingPage /> else go to dashboard.
•	Asegurar que las rutas de login/registro siguen funcionando (quizá mover login a /login en vez de root).
•	Sistema de Suscripciones y Pagos: Esto es nuevo y no implementado. Probablemente se integrará con algún servicio (Stripe). Para el MVP podría bastar con presentar los planes sin procesar pagos realmente, solo simulando registro. Pero a mediano plazo:
•	Implementar un flujo de compra: en registro, si elige plan individual de pago, dirigir a una página de checkout (Stripe Checkout pre-construido) y tras pago exitoso marcar al usuario con plan "premium".
•	Para institutos, sería más B2B: quizás un contacto comercial o generar un pago por X licencias.
•	Esto involucra decisiones de negocio (precio, límite de cursos, etc.) que deben definirse antes de la implementación técnica.
•	Como mínimo, diseñar el modelo de datos para planes (tal vez en User o Institute añadir campo plan_type, expiry_date, etc.).
•	Aplicación Móvil (React Native): La idea es crear una app complementaria con funciones limitadas:
•	Primera funcionalidad móvil: permitir a profesores sacar fotos de exámenes y corregirlos rápidamente con IA. Esto es lo mencionado en revisión automática, pero en contexto móvil hace total sentido (el profesor en clase toma foto del examen escrito de un alumno y la app le devuelve la nota). Para esto, la app móvil sería principalmente una interfaz de cámara + subida al endpoint de corrección. Gran parte de la lógica de IA residiría en el backend compartido, por lo que hay que diseñar la API para que soporte recibir una imagen (quizá base64 o multipart) y datos de rubrica para devolver resultado.
•	Otras posibles funciones móviles futuras: Que los alumnos puedan consumir los módulos virtuales en la app. Inicialmente dice que solo se enfocarán en la parte de módulos virtuales para alumnos en móvil (lo esencial: leer contenidos, ver videos, contestar quizzes, etc.), mientras que configuración de cursos, generación de contenidos, etc., quedaría solo en web.
•	Así, la app móvil podría tener dos perfiles: Profesor (solo funcionalidad de escanear/corregir por ahora) y Alumno (ver módulos). Habría que exponer los mismos endpoints JWT para la app.
•	Tecnológicamente, conviene usar React Native para reusar código UI en lo posible (componentes estilo React web adaptados a RN). Quizá se puede compartir lógica de estado, servicios API, etc., a través de una librería común. Este es un proyecto en sí mismo, se sugiere planificarlo tras consolidar la web.
•	Sidebar de Estudiante (Corrección): Actualmente hay un bug: en el menú lateral del estudiante, solo aparece el nombre del curso pero no despliega los módulos ni temas. Esto confunde la navegación del alumno.
•	Debemos corregir la consulta que llena ese menú. Posiblemente deba obtener todos los VirtualModules del estudiante para ese curso. El backend ya tiene get_student_modules(study_plan_id, student_id)[99][100] que devuelve los módulos virtuales del estudiante para un plan. El front debe usarla para listar los módulos dentro del curso.
•	Luego, cada módulo tendrá sus topics (VirtualTopics) – se podrían listar anidados o cargar bajo demanda al expandir. get_module_details también devuelve los topics virtuales ya generados[101][102]. Quizá es más eficiente obtenerlos de ahí.
•	Solución: Revisar el componente Sidebar para asegurarse de que tras login del estudiante, carga su(s) cursos (planes de estudio asignados) y para cada uno sus módulos virtuales (al menos los iniciados). Podríamos también mostrar módulos bloqueados (no iniciados aún) con candado para que sepa qué viene.
•	Esto mejora la usabilidad, ya que actualmente no ve su progreso global.
•	Dashboard con estadísticas reales: Hay un panel de inicio para profesor y estudiante con indicadores (asistencias, rendimiento, etc.). Hasta ahora, mucho era estático o maqueta. Con la implementación de ContentResults y evaluaciones, podemos poblar esos indicadores:
•	Para estudiantes: Progreso general (porcentaje de módulos completados, o calificación promedio), Próximas tareas (evaluaciones con due_date próximas), Racha de días estudiando (si calculamos días con actividad).
•	Para profesores: Calificaciones promedio por curso, Porcentaje de contenidos generados completados por los alumnos, Asistencia (solo si lleváramos registro de asistencias).
•	Respecto a asistencia, actualmente no hay una funcionalidad para que el profesor marque asistencia diaria. Si no se planea implementar, mejor remover ese widget para no mostrar datos vacíos. O se podría repensar: quizás usar la interacción con módulos virtuales como proxy de asistencia (no es lo mismo que presencia física, pero en e-learning se puede interpretar).
•	Otra métrica: Uso de la plataforma (número de contenidos vistos, número de preguntas respondidas correctamente, etc.).
•	Estas estadísticas requieren agregar consultas en analytics/services.py o dashboards/services.py. Por ejemplo, conteo de ContentResults por alumno para calcular calificación promedio (tomando solo quizzes con use_quiz_score?).
•	También, se puede mostrar en dashboard del alumno una visualización de su perfil de aprendizaje (porcentaje visual, auditivo, etc. basado en su test o su comportamiento).
Primera vs Nueva Prioridad: Finalmente, el usuario pregunta qué priorizar: consolidar lo existente vs nuevas funciones. Claramente, lo existente (módulos virtuales y evaluaciones) es el corazón a estabilizar antes de sumarle expansiones como marketplace, móvil, etc. Así que: - Prioridad 1: Terminar módulos virtuales, personalización, content results y evaluaciones (secciones 1–5 de este análisis). Esto asegura que el sistema cumple su promesa básica de enseñar de forma personalizada y evaluar el progreso. - Prioridad 2: Añadir funcionalidades complementarias que mejoran la experiencia inmediata: corrección de bugs (sidebar), mejoras UI (responsividad, dark mode), autenticación email, etc., para lanzar una versión usable. - Prioridad 3: Las grandes extensiones: revisión automática de exámenes, marketplace de cursos/plantillas, app móvil, multi-instituto, suscripciones de pago. Estas pueden planificarse en fases posteriores sin bloquear el uso principal.
A continuación, se presenta un Plan de Implementación por fases, detallando tareas de backend (B) y frontend (F) para abordar todo lo anterior de forma ordenada.
Plan de Implementación por Fases
Fase 1: Consolidación de Módulos Virtuales y Evaluaciones (Prioridad Máxima)
Objetivo: Tener la experiencia de aprendizaje adaptativo completamente funcional y estable, incluyendo progresión automática y evaluaciones básicas.
•	B: Finalizar ContentResult integration: Implementar endpoints para crear ContentResult al completar un quiz o juego. Asegurar que al marcar contenido completado se actualice VirtualTopicContent.status y VirtualTopic.progress. Unificar si posible la inserción de notas manuales y de entregas en ContentResult[27].
•	B: Sistema de Evaluaciones (mínimo viable):
•	Endpoint para crear/editar Evaluations (si no existe completo). Validar la combinatoria de flags use_quiz_score, etc.
•	Endpoint para que profesor registre manualmente una calificación (ej. POST /api/evaluation/<id>/grade con student_id, score, feedback -> crea ContentResult).
•	Endpoint para entrega de archivo: estudiante sube archivo (usar ya /api/resources), luego POST /api/evaluation/<id>/submit crea EvaluationResource role "submission".
•	Al obtener Evaluations de un módulo, incluir si hay submission pendiente para ese estudiante y, si calificada, el score (uniendo con ContentResult).
•	B: IA Corrección Automática (backbone): Implementar un servicio asíncrono (puede ser función dummy primero) para procesar entregas:
•	Detectar nuevas EvaluationResource submissions. Esto podría ser con un campo status en EvaluationResource o simplemente al momento de submit, llamar a una función.
•	Por ahora, simular que lee el archivo y asigna score 1.0 (para probar flujo). Más adelante integrar OCR + LLM.
•	Guardar ContentResult con ese score y feedback “Calificado automáticamente”.
•	B: Sincronización de cambios:
•	Llamar synchronize_module_content en lugares adecuados (por ejemplo, cada vez que el estudiante abre un módulo o se genera nuevos temas). Agregar filtro para no devolver contenidos archived al estudiante[103].
•	Implementar detección de cambios en contenidos: por simplicidad, si un TopicContent.updated_at es posterior a la creación del VirtualTopicContent, actualizar su content en sincronización (posiblemente reemplazar adapted_content o marcar para recrear). Este subcomponente podría ser complejo (p.ej., si se personalizó antes, volver a personalizar). Quizá no priorizar si no es común que maestros editen después de publicar.
•	F: Frontend Módulos Virtuales:
•	Consumir GET /api/virtual/topic/<id>/contents para llenar la vista del tema (ya hecho).
•	Implementar llamada a POST /api/virtual/trigger-next-topic cuando progress >= 0.8. El componente que trackea progreso del tema (quizás al pasar página final) debe dispararlo y manejar la respuesta: si genera nuevos temas, actualizar el estado global de temas (por ej., refrescar la lista de temas del módulo en sidebar).
•	Mejorar Sidebar del estudiante: usar get_student_modules y anidar módulos/temas. Mostrar candados en temas con locked=true.
•	Indicar progreso en UI: por tema (barra o porcentaje junto a cada tema) y general del módulo.
•	Evaluaciones UI (versión básica):
o	En vista del módulo del alumno, listar evaluaciones (nombre, peso, due_date). Si use_quiz_score y el linked quiz ya está en un tema, puede un botón “Ir al Quiz” o automáticamente tomar ese ContentResult cuando exista.
o	Si requires_submission, mostrar formulario para subir archivo (reusar FileUpload comp.) y luego status "Entregado" o "Calificado: X".
o	En vista del módulo del profesor, listar evaluaciones definidas; para cada una, si manual, permitir ingresar notas (un botón "Calificar" que abre lista de alumnos del curso, introduciendo nota por cada, generando ContentResult).
o	Mostrar rubricas/archivos adjuntos: enlace para que alumno descargue el enunciado (EvaluationResource template).
o	Notificar cuando llega una corrección automática (esto puede ser vía polling: alumno consulta su nota hasta ver reflejado).
•	Toasts de Generación: Implementar toasts persistentes para generación de contenidos de profesor: cuando inicia generación de un tema, mostrar "Generando [Texto, Quiz, Diagrama]..." con actualizaciones. Si la página recarga, al montar, chequear si algún TopicContent está en status “generating” (posible campo a añadir) y re-lanzar toasts. Opcional en esta fase.
•	Dark mode & responsive: Auditar las páginas principales (login, dashboard, module view, content viewer) en modo oscuro y en móvil. Ajustar clases o estilos donde se rompan (p.ej., textos negros en dark mode). Hacer pruebas manuales.
•	Errores UX: Implementar un <ErrorBoundary> global para capturar excepciones de render y mostrar un mensaje amigable en vez de pantalla en blanco. Y un manejo global de fetch errores: si un request da 401 (token expirado), logout al login; si 500, toast "Error del servidor"; si 403, toast "No autorizado", etc.
Fase 2: Mejora de Plataforma y Usuarios (Autenticación, Multi-rol, UI polish)
Objetivo: Pulir la plataforma para un entorno multi-usuario real, con distintos roles y robustez en flujos de acceso.
•	B: Autenticación email/contraseña:
•	Agregar campos password_hash a User (si no existe).
•	Endpoint POST /api/users/register que acepte email, password, role (estudiante, profesor, etc., según caso de uso de auto registro para individuales), cree el usuario (hashed password). Quizá para individual, crear también institute/clase (ver más adelante).
•	Endpoint POST /api/users/login (creo que existe uno para Google, adaptarlo): validar credenciales, generar JWT.
•	Endpoint POST /api/users/forgot-password que genere token temporal y envíe email (esta última parte requiere config SMTP o servicio email – tal vez usar SendGrid API). Y POST /api/users/reset-password para aplicar nuevo password con token válido.
•	Asegurar compatibilidad: los usuarios existentes de Google no tendrán password; permitir login social paralelo (puede seguir como ahora).
•	F: Autenticación email UI:
•	Pantalla de registro donde usuario elige tipo de cuenta (Institución, Profesor, Estudiante individual). Según elección, pedir datos: Institución (nombre inst., admin email, etc.), Profesor (sus datos personales, institución = quizás "Personal"), Estudiante (sus datos y tal vez si se va a autogestionar). Este es un poco complejo, ver Multi-instituto.
•	Pantalla de login con opción email/pass además de botón Google.
•	Pantalla de recuperar contraseña (ingresa email, muestra mensaje de email enviado; pantalla de reset con campos).
•	Actualizar fetchWithAuth o contexto auth para manejar login email (almacenar JWT igual que con Google).
•	B: Multi-Instituto para profesores: Actualmente, un profesor está ligado a un institute_id en su JWT supongo. Para soportar varios:
•	Permitir que user.memberships exista (ya lo hay con InstituteMember). Quizá al login, si hay >1 InstituteMember, devolver esa lista.
•	Endpoint para cambiar contexto: e.g. POST /api/users/switch-institute/<inst_id> que emita un nuevo token JWT con ese instituto como activo (o sin institute fijo pero con claims de multiples roles? Esto puede ser complejo, más fácil reemitir token con single contexto).
•	O en lugar de embutir inst_id en token, cambiar lógica: usar siempre user_id en token y filtrar datos por membership en consultas. Eso implicaría que en cada request se sepa qué instituto se quiere. Podríamos derivarlo del path (e.g., /api/institute/<id>/courses...). Alternativamente, mantener el approach actual pero obligar al usuario a “seleccionar instituto activo” en frontend y guardarlo en localStorage, luego incluirlo como header en peticiones.
•	Dado el tiempo, quizás más simple: si es multi-institute, implementar front que tras login, si detecta múltiples InstituteMember, le pida seleccionar uno (listando nombres inst.). Al elegir, guardamos su institute_id en contexto (no necesariamente regenerar token, a menos que se use en claims; si se usa, entonces necesitaríamos re-login). Podemos quizá no meterlo en JWT y en su lugar, filtrar a nivel de consulta con un parámetro, pero eso requiere revisar muchos servicios.
•	Dado que multi-institute es más bien para el caso de un profesor freelance en varias organizaciones, se puede posponer un poco.
•	Alternativa provisional: Permitir crear usuarios duplicados con correos distintos por cada instituto. Esto no es ideal, pero podría ser una salida sencilla si la funcionalidad no es muy demandada ahora.
•	F: Selector de instituto:
•	Si implementamos lo anterior, proveer UI al profesor para cambiar entre contextos (por ejemplo, en el header mostrar inst. actual con dropdown de otras).
•	Ajustar las consultas en frontend para incluir el filtro de institute (por ejemplo, cuando pida cursos, que sea solo los de ese inst.).
•	UI/UX Pulido:
•	Revisar textos, labels y mensajes para uniformidad. E.g., asegurarse de traducir o presentar en español consistente (algunos logs están en español, lo cual es bueno).
•	Mejorar componentes de carga (spinners) para operaciones lentas (generaciones IA, etc.).
•	Confirmar que todas las rutas relevantes requieren login. Si usando Next.js, implementar middleware que redirija si no hay token.
•	Dashboard real data: Implementar llamadas para obtener estadísticas:
o	Backend: en dashboards/services.py crear funciones para calcular, por ejemplo: progreso promedio del alumno (lo puede sacar de VirtualModule.progress o calculándolo de VirtualTopics completados), mejores puntajes, etc., y retornarlo.
o	Frontend: Llamar esos endpoints y mostrar en las tarjetas.
o	Para estudiante: calificación promedio = promedio ContentResults de evaluaciones formales; progreso = (temas completados / total) 100; o tiempo de estudio* (sumando metrics.time_spent).
o	Para profesor: promedio de clase en última evaluación, % alumnos completando módulos a tiempo, etc.
o	Si no da tiempo de hacer cálculos complejos, mostrar al menos contadores básicos (nº de cursos, nº de módulos en curso, etc.) para que no esté vacío.
•	Notificaciones: Esto no se mencionó, pero podría ser útil notificar:
•	Al estudiante: “Nuevo módulo desbloqueado”, “Evaluación X calificada con nota Y”, “Tienes una evaluación mañana”.
•	Al profesor: “X entregó su proyecto”, “La IA ha calificado la entrega de Y, revise aquí”.
•	Implementar notificaciones puede hacerse simple con un collection/endpoint de notifications o usando un servicio push. Dado el alcance, se puede posponer, pero tenerlo en cuenta en diseño UI (tal vez un ícono campana con conteo).
Fase 3: Extensiones Avanzadas (Marketplace, Auto-Corrección IA completa, App Móvil)
Objetivo: Desarrollar las funciones de valor añadido una vez que la base del sistema funciona. Esto incluye herramientas de IA avanzadas y apertura de la plataforma a la comunidad.
•	B: Revisión Automática Exámenes (completa):
•	Integrar API OCR (Google Vision, Tesseract) para extraer texto de imágenes de entregas. Probablemente hacerlo en backend: recibir imagen, usar librería de OCR, obtener texto.
•	Preparar prompt y llamar a modelo de lenguaje con ese texto y rubrica. Podríamos usar GPT-4 via OpenAI API, dado su capacidad de análisis. Incluir en prompt: “Eres un corrector. La respuesta del estudiante es: [texto]. La rúbrica: [p.ej. 10 puntos, resta 0.5 por falta ortográfica, etc.]. Evalúa del 0 al 10 y da feedback.” Recibir respuesta, parsear score y feedback.
•	Actualizar ContentResult con estos datos.
•	Manejar errores: si OCR falla (imagen ilegible), marcar como no pudo auto-corregir y avisar al profesor que debe hacerlo manual.
•	Este proceso puede tardar varios segundos, es ideal aislarlo en un job asíncrono con notificación al completar (podemos usar websockets o simplemente que el front haga polling del estado de la entrega hasta ver un ContentResult aparecer).
•	Seguridad: controlar que la imagen pertenezca a ese evaluation y user.
•	F: Interfaz de Revisión IA:
•	En la vista del profesor, para cada entrega puede mostrar “Corrección automática: [score y feedback]. ¿Aceptar?” con opción de modificar. Si acepta, finaliza; si modifica, guarda cambios (actualiza ContentResult).
•	En la app móvil, implementar captura: profesor toma foto y selecciona a qué Evaluación (y alumno) corresponde. Podría ser mediante scan de código de estudiante o selección manual. Para MVP móvil, quizás más sencillo: tener la lista de evaluaciones pendientes y dentro lista de alumnos sin nota -> abrir cámara para cada uno.
•	La app luego envía la imagen al endpoint de corrección, espera respuesta o la obtiene más tarde. Se puede mostrar la nota en pantalla móvil para confirmación.
•	B: Marketplace de Cursos:
•	Permitir marcar un plan de estudio como “público” o “compartido”. Quizás un campo StudyPlan.is_public = True.
•	Endpoint para listar cursos públicos (con info: título, nivel, autor/profesor).
•	Posiblemente permitir calificación/comentarios de cursos (social aspect), pero se puede omitir inicialmente.
•	Endpoint para que un usuario individual se suscriba a un curso público: Esto implicaría:
o	Crear un VirtualModule (o varios) para ese usuario basados en el plan seleccionado. El usuario no perteneciendo a la institución original, pero como es público, podríamos permitirlo.
o	Alternativamente, copiar el StudyPlan bajo el instituto “Academia Sapiens” del usuario. Eso es costoso (duplicar todos los contenidos). Quizás mejor: el VirtualModule puede referenciar el módulo original sin que el estudiante pertenezca al inst original. Habría que relajar la verificación de permisos en check_study_plan_exists y fetch de contenidos para estos casos.
o	Quizá crear un pseudo-institute global para cursos públicos donde todos pueden acceder. En cuyo caso, al suscribirse, simplemente asignarle VirtualModules con student_id del usuario en ese pseudo-institute context.
•	Este es un punto complejo, se puede posponer su detalle e inicialmente permitir solo descarga del plan (ej.: exportar a PDF) en vez de integrarlo en la plataforma. Sin embargo, la idea de marketplace es fuerte, vale la pena planearlo bien.
•	F: Marketplace de Plantillas de Juegos:
•	Modelo y endpoints para GameTemplate (como discutido en sección 9).
•	UI en el módulo de profesor: sección "Plantillas de Juego", mostrando los disponibles con botón “Generar contenido con esta plantilla”.
•	Al click, pedir algunos inputs (quizá temas o datos) y luego crear un TopicContent de tipo game usando la plantilla (posiblemente clonando el Resource del juego y reemplazando textos).
•	Permitir previsualización del juego antes de usarlo.
•	También permitir a profesores publicar una plantilla que crearon (un toggle "Compartir esta plantilla con la comunidad").
•	Este subsistema es grande; se puede planear como sub-proyecto, quizás en paralelo con desarrollo de app móvil, ya que no es crítico para funcionalidad base.
•	B: Multi-rol individual (Profesor y Alumno simultáneo): Para soportar usuarios individuales que crean sus propios cursos y a la vez los “cursan”, se podría:
•	En registro del plan Profesor Individual, automáticamente crear una clase donde él es el único estudiante también. Alternativamente, crear dos cuentas vinculadas (complicado).
•	Una manera sencilla: marcar el usuario con roles duales en InstituteMember y ClassMember. Ej.: Crear un instituto “Personal de {nombre}”, un curso en él, y añadir el user como InstituteMember (role TEACHER) y ClassMember (role STUDENT) de su propia clase.
•	Actualizar frontend para que estos usuarios tengan acceso tanto a vistas de profesor (para generar su curso) como de alumno (para cursarlo). Podría ser un toggle "Modo Profesor/Modo Alumno" en la interfaz.
•	Esto es retador de UX pero posible: por ejemplo, al entrar un usuario individual, mostrarle primero la interfaz de creación de plan (profesor), y tras generarlo, un botón "Iniciar curso como estudiante" que lo lleva a la vista del alumno de ese plan.
•	En backend, al generar el plan de estudio, asignarlo a la clase del mismo user para que tenga VirtualModules generados.
•	Asegurar que los endpoints de generar módulos virtuales funcionen aun si el user es también teacher (de hecho, VirtualModule genera para cualquier user id, no depende del rol).
•	Se debe probar que no haya conflictos (por ej., no debería contarse a sí mismo dos veces en calificaciones, etc., pero siendo único no hay lío).
•	F: Landing Page & Suscripciones:
•	Diseñar la landing con marketing content. Posiblemente ya existe un diseño. Incluir testimonios, features, etc.
•	Botón “Regístrate” que abra opciones: Institución (dirige a formulario contacto comercial o un registro admin directo), Profesor Particular (registro normal), Alumno Particular (registro normal).
•	Integrar un sistema de pago: Podríamos para prototipo no hacerlo real, sino tras registro de plan particular, marcarlo como trial. Si se desea pagos, integrar Stripe:
o	Crear precios (Stripe Products) para cada plan (mensual, anual).
o	En registro, tras llenar datos, redirigir a Stripe Checkout. Al volver (webhook or return URL), crear el usuario/instituto según pago confirmado.
o	Esto requiere backend para manejar webhooks (e.g., POST /api/stripe/webhook).
o	No es trivial y puede posponerse hasta tener usuarios activos que lo requieran. Quizá para demos se asume todo free.
•	B: Asistencia y otros indicadores: Si se decide implementar control de asistencia (p.ej., para institutos que llevan registro de clase presencial), se necesitaría:
•	Modelo Attendance (class_id, date, present_student_ids...).
•	UI para profesor marcar daily attendance.
•	Este es más propio de un LMS tradicional que de este enfoque autodidacta, así que puede ser depriorizado. Por ahora, se puede ocultar ese widget o reemplazarlo con otro (por ejemplo, un gráfico de avance del curso).
•	F: App Móvil React Native:
•	Configurar proyecto RN (bare workflow o Expo si se prefiere).
•	Implementar login (usar los mismos endpoints JWT).
•	Pantalla para profesor: lista de sus evaluaciones pendientes de calificar (fetch /api/evaluations?pending=true).
•	Dentro de evaluación: lista de estudiantes sin nota. Al seleccionar uno, abrir cámara (usar expo-camera), tomar foto, llamar endpoint de upload entrega (podríamos reutilizar el mismo /submit que viene del web, o crear uno específico para calificar offline).
•	Recibir resultado AI, mostrarlo. Permitir "Guardar" para confirmar nota final.
•	Pantalla para alumno: lista de sus módulos virtuales -> temas -> contenidos (presentarlos quizás en webview simplificado o recrear UI en RN). Quiz, lecturas, etc., se pueden presentar nativo (pero es mucho reimplementación). Quizá inicialmente no implementamos full alumno en móvil, solo la parte de correcciones IA.
•	Alternativamente, embedder la web: podría usar WebView to show content (no ideal UX).
•	Dado que es fase 3, se puede empezar con una pequeña POC para la funcionalidad de foto y corrección, que es la de mayor valor en móvil.
•	Otros (Menores):
•	Integración con servicios externos: por ej., Google Classroom (si se quisiera importar alumnos) – no se mencionó pero a futuro podría ser plus.
•	SEO para la landing pública (meta tags, etc. para atraer usuarios individuales).
•	Analytics (tracking de uso) – no crítico ahora.
El plan anterior, en resumen, prioriza primero asegurar que el sistema actual cumple con su promesa (módulos virtuales adaptativos y evaluaciones), luego perfeccionar la experiencia de usuario y acceso, y finalmente expandir con innovaciones de IA y comunidad. Con este roadmap detallado, el desarrollo podrá encajar las nuevas funcionalidades de manera lógica reutilizando componentes existentes siempre que sea posible, y evitando refactorizaciones drásticas. De esta forma, consolidamos lo ya construido y añadimos las extensiones de la manera más eficiente y coherente con la arquitectura actual.
